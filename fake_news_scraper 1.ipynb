{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Setup\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "url = \"https://www.bbc.com/news/topics/cjxv13v27dyt\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "all_articles = []\n",
    "\n",
    "def extract_articles(soup):\n",
    "    articles = soup.find_all(\"div\", {\"data-testid\": \"liverpool-card\"})\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            link_tag = article.find(\"a\", {\"data-testid\": \"internal-link\"})\n",
    "            href = link_tag['href']\n",
    "            full_link = \"https://www.bbc.com\" + href\n",
    "\n",
    "            title = article.find(\"h2\", {\"data-testid\": \"card-headline\"}).get_text(strip=True)\n",
    "\n",
    "            region_tag = article.find(\"span\", {\"data-testid\": \"card-metadata-tag\"})\n",
    "            region = region_tag.get_text(strip=True) if region_tag else None\n",
    "\n",
    "            date_tag = article.find(\"span\", {\"data-testid\": \"card-metadata-lastupdated\"})\n",
    "            date = date_tag.get_text(strip=True) if date_tag else None\n",
    "\n",
    "            results.append({\n",
    "                \"title\": title,\n",
    "                \"link\": full_link,\n",
    "                \"region\": region,\n",
    "                \"date\": date\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing article:\", e)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Reached last page.\n"
     ]
    }
   ],
   "source": [
    "# Pagination loop\n",
    "page = 1\n",
    "while True:\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    all_articles.extend(extract_articles(soup))\n",
    "\n",
    "    try:\n",
    "        next_button = driver.find_element(\"xpath\", \"//button[@data-testid='pagination-next-button']\")\n",
    "        # Check if it's disabled\n",
    "        if next_button.get_attribute(\"disabled\"):\n",
    "            print(\"Reached last page.\")\n",
    "            break\n",
    "        next_button.click()\n",
    "        time.sleep(2)\n",
    "        page += 1\n",
    "    except (NoSuchElementException, ElementClickInterceptedException):\n",
    "        print(\"No more pages or click failed.\")\n",
    "        break\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_articles).drop_duplicates(subset=\"link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(276, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>region</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Google AI presented my April Fools' story as ...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cly12egqq5ko</td>\n",
       "      <td>Wales</td>\n",
       "      <td>3 Apr 2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Woman sentenced in case that sparked Springfie...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cy890gpqw1po</td>\n",
       "      <td>US &amp; Canada</td>\n",
       "      <td>3 Dec 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Onion buys Alex Jones's Infowars at auction</td>\n",
       "      <td>https://www.bbc.com/news/articles/c30p1p0j0ddo</td>\n",
       "      <td>US &amp; Canada</td>\n",
       "      <td>14 Nov 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How US election fraud claims changed as Trump won</td>\n",
       "      <td>https://www.bbc.com/news/articles/cy9j8r8gg0do</td>\n",
       "      <td>US &amp; Canada</td>\n",
       "      <td>8 Nov 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Whirlwind of misinformation sows distrust ahea...</td>\n",
       "      <td>https://www.bbc.com/news/articles/czj7eex29r3o</td>\n",
       "      <td>Technology</td>\n",
       "      <td>3 Nov 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>What claims do you want BBC Reality Check to i...</td>\n",
       "      <td>https://www.bbc.com/news/uk-41928747</td>\n",
       "      <td>UK</td>\n",
       "      <td>17 Jan 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>Russia bans 'disrespect' of government</td>\n",
       "      <td>https://www.bbc.com/news/world-europe-47488267</td>\n",
       "      <td>Europe</td>\n",
       "      <td>7 Mar 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>QAnon: What's the truth behind a pro-Trump con...</td>\n",
       "      <td>https://www.bbc.com/news/blogs-trending-45040614</td>\n",
       "      <td>BBC Trending</td>\n",
       "      <td>2 Aug 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>BBC game challenges young people to spot \"fake...</td>\n",
       "      <td>https://www.bbc.com/news/school-report-43391188</td>\n",
       "      <td>Family &amp; Education</td>\n",
       "      <td>14 Mar 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>The (almost) complete history of 'fake news'</td>\n",
       "      <td>https://www.bbc.com/news/blogs-trending-42724320</td>\n",
       "      <td>BBC Trending</td>\n",
       "      <td>21 Jan 2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    'Google AI presented my April Fools' story as ...   \n",
       "1    Woman sentenced in case that sparked Springfie...   \n",
       "2      The Onion buys Alex Jones's Infowars at auction   \n",
       "3    How US election fraud claims changed as Trump won   \n",
       "4    Whirlwind of misinformation sows distrust ahea...   \n",
       "..                                                 ...   \n",
       "271  What claims do you want BBC Reality Check to i...   \n",
       "272             Russia bans 'disrespect' of government   \n",
       "273  QAnon: What's the truth behind a pro-Trump con...   \n",
       "274  BBC game challenges young people to spot \"fake...   \n",
       "275       The (almost) complete history of 'fake news'   \n",
       "\n",
       "                                                 link              region  \\\n",
       "0      https://www.bbc.com/news/articles/cly12egqq5ko               Wales   \n",
       "1      https://www.bbc.com/news/articles/cy890gpqw1po         US & Canada   \n",
       "2      https://www.bbc.com/news/articles/c30p1p0j0ddo         US & Canada   \n",
       "3      https://www.bbc.com/news/articles/cy9j8r8gg0do         US & Canada   \n",
       "4      https://www.bbc.com/news/articles/czj7eex29r3o          Technology   \n",
       "..                                                ...                 ...   \n",
       "271              https://www.bbc.com/news/uk-41928747                  UK   \n",
       "272    https://www.bbc.com/news/world-europe-47488267              Europe   \n",
       "273  https://www.bbc.com/news/blogs-trending-45040614        BBC Trending   \n",
       "274   https://www.bbc.com/news/school-report-43391188  Family & Education   \n",
       "275  https://www.bbc.com/news/blogs-trending-42724320        BBC Trending   \n",
       "\n",
       "            date  \n",
       "0     3 Apr 2025  \n",
       "1     3 Dec 2024  \n",
       "2    14 Nov 2024  \n",
       "3     8 Nov 2024  \n",
       "4     3 Nov 2024  \n",
       "..           ...  \n",
       "271  17 Jan 2020  \n",
       "272   7 Mar 2019  \n",
       "273   2 Aug 2018  \n",
       "274  14 Mar 2018  \n",
       "275  21 Jan 2018  \n",
       "\n",
       "[276 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        text_blocks = soup.find_all(\"div\", {\"data-component\": \"text-block\"})\n",
    "        paragraphs = []\n",
    "        for block in text_blocks:\n",
    "            for p in block.find_all(\"p\"):\n",
    "                paragraphs.append(p.get_text(strip=True))\n",
    "        \n",
    "        return \" \".join(paragraphs)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every year, journalist Ben Black publishes a playful fake story on his community news site Cwmbran Life for April Fools\\' Day. Since 2018 the 48-year-old has spun yarns including a Hollywood-style sign on a mountain to a nudist cold-water swimming club at a lake. In 2020, Mr Black published afake story claiming Cwmbran had been recognised by Guinness World Records for having the most roundabouts per square kilometre. Despite altering the wording of his article that afternoon, when he searched for it on 1 April he said he was \"shocked\" and \"worried\" to find the false information being used by Google\\'s AI tool and presented as real information. Google said it was looking into the matter. Mr Black decided to begin writing fake stories for April Fools\\' Day for \"a bit of fun\" and said his wife usually helped him come up with the ideas. The concept for his story in 2020 came from Cwmbran being a new town, where \"often linking houses with roundabouts is the easiest way to build\". \"I made up a number for the roundabouts per square kilometre and added a fake quote from a resident and clicked publish. \"It went down really well from memory, people laughed,\" he said. That afternoon, Mr Black marked the story as an April Fools\\' prank to clarify it was not fake news to his readers. However, the next day, he was \"annoyed\" to find it had been picked up by a larger national news website without his permission, and despite efforts to try and get the story removed, the story is still online. Mr Black said he had \"forgotten all about it\" until he searched for his previous stories on April Fools\\' Day this year. He said he was surprised to discover the Google AI tool and a learning to drive website using his article to claim Cwmbran reportedly had the world\\'s highest concentration of roundabouts. He said: \"It\\'s really scary that someone in Scotland could Google \\'roads in Wales\\' and come across a story that just isn\\'t true. \"It\\'s not a dangerous story, but it shows how fake news can easily spread even if it\\'s from a trusted news source. \"Even though I changed it all the same day, it shows down the line the internet can do it\\'s own thing. It\\'s just crazy.\" Mr Black said AI was becoming a growing threat to independent publishers, with many tools using their original content without permission and presenting it in different formats for others to benefit from. \"It is really frustrating because now no-one will visit our websites,\" he said. He also pointed out that larger news websites had struck deals with AI firms for collaboration, but said: \"There was no chance I\\'d be able to do that.\" Although Mr Black decided not to publish a fake story for April Fools\\' Day this year because he was \"too busy\", he said the experience had put him off and made him decide not to publish a fake story again.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_article_text('https://www.bbc.com/news/articles/cly12egqq5ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the build-up to Tuesday’s US election, claims of voter fraud flooded social media - but as Donald Trump’s victory crystallised, the chatter largely subsided. The claims didn’t stop entirely, however. A number of right-wing influencers and organisations pushing stories about “cheating” and a “rigged” vote pointed to incomplete vote totals and continued to repeat discredited theories about the 2020 election. And disappointed Democratic Party supporters developed their own unsubstantiated voter fraud theories, some of which went viral on X, formerly Twitter, and other platforms. The reach of the posts is nowhere near the deluge of content that circulated after Trump lost the 2020 election. And with no support from losing candidate Kamala Harris or other Democratic Party officials, the chances seem slim of a large-scale movement developing along the lines of the “Stop the Steal” drive four years ago, which culminated in a riot at the US Capitol. The BBCtracked a huge wave of pre-election fraud claimsthat carried throughelection day and into the evening. These included claims of the vote being “stolen” in some key swing states, with exaggerated takes on real events being used in some cases to bolster the allegations. Early on election day in Cambria County, Pennsylvania, a Republican stronghold, there were problems with voting machines malfunctioning. The issues were fixed and voting hours in the affected areas were extended. However, many online immediately used the story to suggest nefarious activities were taking place. One post at 08:45 local time on Tuesday said: “The election steal is happening!” Other rumours were spread in posts that popped up throughout the day, including one at around 14:00, which claimed ballots in Delaware County, Pennsylvania, had been pre-marked for Kamala Harris. In Milwaukee, the biggest city in the swing state of Wisconsin, elections officials made a decision to re-run around 30,000 ballots out of an “abundance of caution”, after doors on the back of voting machines were left open. Once the count was completed, it showed that support for Harris had dropped compared with Joe Biden\\'s four years earlier. Like many of the pro-Trump posters, Harris supporters pointed to real but isolated events - fires at ballot drop boxes in Washington and Oregon, and a series of fake bomb threats that disrupted voting at several polling locations on election day - as evidence of widespread voter fraud. However, there’s no evidence that the incidents significantly altered the vote or changed the outcome. Several posts from Democratic Party activists questioning the result went viral and were seen by millions on X and other platforms. Pam Keith, a Harris supporter in Florida, posted: “Is it possible that the machines were hacked to switch the tallies from Harris to Trump?” Her message was seen more than one million times on X, according to the site’s metrics. The BBC has reached out to her for comment. Unlike Trump’s campaign in 2020, however, the Harris campaign and top Democratic Party officials have not endorsed allegations of cheating or voter fraud. On election day, fraud rumours also came from President-elect Trump himself, who has repeatedly argued from the outset of his political career that the voting system is unfairly stacked against him. Just after 16:30 Trump posted on his social media platform Truth Social: “A lot of talk about massive CHEATING in Philadelphia. Law Enforcement coming!!!” The now president-elect did not give any details and the Philadelphia Police Department told BBC Verify they were not aware of what Trump was referring to. Seth Bluestein, the Republican City Commissioner in Philadelphia, posted on X: \"There is absolutely no truth to this allegation. It is yet another example of disinformation. Voting in Philadelphia has been safe and secure.\" Trump has not repeated the fraud allegations since election day. We have contacted several hugely influential accounts that regularly posted about election fraud claims in the build-up to the vote, but none of them replied. With data firm NodeXL, the BBC tracked accounts that engaged with Donald Trump, Donald Trump Jr, Eric Trump, Lara Trump and Elon Musk on X around election day. Posts mentioning vote fraud peaked at 15:00 EST on 5 November - but then dropped off significantly that evening and into the next day as polls closed and results came in.  However, some organisations and activists who promoted voter fraud allegations in the past continued to repeat debunked rumours even after the results became clear. Emerald Robinson, a former reporter with right-wing TV networks and a pro-Trump influencer with more than 750,000 followers on X, insisted that Democrats were “cheating right now” and posted: “I always told people the voting machines were rigged!” More generally, reaction from pro-Trump groups and influencers who previously hyped up vote fraud claims varied - from silence on the issue, to continued insistence that the 2020 vote was marred by fraud. The BBC contacted Ms Robinson for comment. In another case, a chart that was widely circulating online claimed to show a sharp drop-off in vote totals in 2024 compared to 2020. Many are pointing to the figures as “proof” of fraud. Conservative commentator Dinesh D\\'Souza, a Trump supporter who has pushed voter fraud theories, posted the day after the election: \"Kamala got 60 million votes in 2024. Does anyone really believe Biden got 80 million in 2020? Where did those 20 million Democratic voters go? The truth is, they never existed.\" However, the chart and the figures circulating online were based on preliminary vote totals, which continue to go up as final results are still being tabulated. Already, Harris has more than 69 million votes in her column - with Trump on more than 73 million. As of Friday, fewer than two million ballots have yet to be counted nationally, in states including Arizona and California, according to Reuters. The BBC contacted Mr D\\'Souza for comment. Those same numbers are also fuelling conspiracy theories from supporters of Harris, who are wondering where their “missing” voters are - and ignoring the fact that turnouts and preferences frequently shift, often dramatically, between elections. Partisans on both sides are also pointing to differences in vote tallies for Harris and other Democrats running for Senate seats. But there is no requirement for US voters to support candidates from just one party, and “ticket-splitting” - voting for candidates from different parties in different races - although becoming rarer, is fairly common in American politics. TheUniversity of Florida’s Election Lab turnout trackeris showing slightly lower turnout in 2024 as compared to 2020 - 62.5% v just over 66%. Additional reporting by Shayan Sardarizadeh and Merlyn Thomas What do you want BBC Verify to investigate?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_article_text('https://www.bbc.com/news/articles/cy9j8r8gg0do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the dataframe by adding full article content\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_text(url):\n",
    "    \"\"\"Extract full article text from BBC article URL\"\"\"\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Try multiple selectors for BBC article content\n",
    "        text_blocks = soup.find_all(\"div\", {\"data-component\": \"text-block\"})\n",
    "        paragraphs = []\n",
    "        \n",
    "        if text_blocks:\n",
    "            for block in text_blocks:\n",
    "                for p in block.find_all(\"p\"):\n",
    "                    text = p.get_text(strip=True)\n",
    "                    if text:\n",
    "                        paragraphs.append(text)\n",
    "        else:\n",
    "            # Fallback to other common BBC selectors\n",
    "            article_body = soup.find(\"div\", {\"class\": \"story-body\"}) or soup.find(\"main\")\n",
    "            if article_body:\n",
    "                for p in article_body.find_all(\"p\"):\n",
    "                    text = p.get_text(strip=True)\n",
    "                    if text and len(text) > 20:  # Filter out very short paragraphs\n",
    "                        paragraphs.append(text)\n",
    "        \n",
    "        return \" \".join(paragraphs) if paragraphs else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 276 articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping articles: 100%|██████████| 276/276 [04:37<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping completed:\n",
      "Successful: 276\n",
      "Failed: 0\n",
      "Success rate: 100.0%\n",
      "Final dataset: 276 articles with content\n",
      "Complete dataset saved to 'bbc_articles_complete.csv' and 'bbc_articles_complete.json'\n",
      "\n",
      "Dataset Statistics:\n",
      "Total articles: 276\n",
      "Average content length: 5266 characters\n",
      "Date range: 1 Feb 2021 to 9 Sep 2024\n",
      "Regions covered: 35\n",
      "\n",
      "Sample article:\n",
      "Title: 'Google AI presented my April Fools' story as real news'\n",
      "Region: Wales\n",
      "Date: 3 Apr 2025\n",
      "Content preview: Every year, journalist Ben Black publishes a playful fake story on his community news site Cwmbran Life for April Fools' Day. Since 2018 the 48-year-old has spun yarns including a Hollywood-style sign...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_complete_dataset(df):\n",
    "    \"\"\"Add full article content to the existing dataframe\"\"\"\n",
    "    print(f\"Processing {len(df)} articles...\")\n",
    "    \n",
    "    # Add content column\n",
    "    df_complete = df.copy()\n",
    "    df_complete['content'] = None\n",
    "    df_complete['content_length'] = 0\n",
    "    df_complete['scraping_status'] = 'pending'\n",
    "    \n",
    "    successful_scrapes = 0\n",
    "    failed_scrapes = 0\n",
    "    \n",
    "    for idx, row in tqdm(df_complete.iterrows(), total=len(df_complete), desc=\"Scraping articles\"):\n",
    "        try:\n",
    "            content = extract_article_text(row['link'])\n",
    "            \n",
    "            if content and len(content.strip()) > 100:  # Ensure meaningful content\n",
    "                df_complete.at[idx, 'content'] = content\n",
    "                df_complete.at[idx, 'content_length'] = len(content)\n",
    "                df_complete.at[idx, 'scraping_status'] = 'success'\n",
    "                successful_scrapes += 1\n",
    "            else:\n",
    "                df_complete.at[idx, 'scraping_status'] = 'failed_no_content'\n",
    "                failed_scrapes += 1\n",
    "            \n",
    "            # Be respectful to the server\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            df_complete.at[idx, 'scraping_status'] = f'failed_error: {str(e)[:50]}'\n",
    "            failed_scrapes += 1\n",
    "            print(f\"Error processing {row['link']}: {e}\")\n",
    "    \n",
    "    print(f\"\\nScraping completed:\")\n",
    "    print(f\"Successful: {successful_scrapes}\")\n",
    "    print(f\"Failed: {failed_scrapes}\")\n",
    "    print(f\"Success rate: {successful_scrapes/(successful_scrapes+failed_scrapes)*100:.1f}%\")\n",
    "    \n",
    "    # Filter to only successful scrapes\n",
    "    df_successful = df_complete[df_complete['scraping_status'] == 'success'].copy()\n",
    "    print(f\"Final dataset: {len(df_successful)} articles with content\")\n",
    "    \n",
    "    return df_successful\n",
    "\n",
    "# Load your existing dataframe (assuming it's saved or recreate from your notebook)\n",
    "# df = pd.read_csv('bbc_articles.csv')  # if you saved it\n",
    "# OR recreate it from your notebook data\n",
    "\n",
    "# For now, I'll assume you have your df from the notebook\n",
    "# Run this after loading your df:\n",
    "df_with_content = create_complete_dataset(df)\n",
    "\n",
    "# Save the complete dataset\n",
    "df_with_content.to_csv('bbc_articles_complete.csv', index=False)\n",
    "df_with_content.to_json('bbc_articles_complete.json', orient='records', indent=2)\n",
    "\n",
    "print(\"Complete dataset saved to 'bbc_articles_complete.csv' and 'bbc_articles_complete.json'\")\n",
    "\n",
    "# Display statistics\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Total articles: {len(df_with_content)}\")\n",
    "print(f\"Average content length: {df_with_content['content_length'].mean():.0f} characters\")\n",
    "print(f\"Date range: {df_with_content['date'].min()} to {df_with_content['date'].max()}\")\n",
    "print(f\"Regions covered: {df_with_content['region'].nunique()}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample article:\")\n",
    "sample = df_with_content.iloc[0]\n",
    "print(f\"Title: {sample['title']}\")\n",
    "print(f\"Region: {sample['region']}\")\n",
    "print(f\"Date: {sample['date']}\")\n",
    "print(f\"Content preview: {sample['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
